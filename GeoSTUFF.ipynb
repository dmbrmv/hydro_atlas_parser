{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to shape file\n",
    "# shape_file_path = '/mnt/d/education/HSI/aspirantura/CAMELS_ru/files/openf_gauges_watersheds/watersheds_openf.shp'\n",
    "shape_file_path = '/home/dima/Documents/education/HSI/aspirantura/Dissertation/conus_data/basin_set_full_res/HCDN_nhru_final_671.shp'\n",
    "# set path to downloaded HydroATLAS\n",
    "gdb_file_path = '/home/dima/Documents/education/HSI/aspirantura/Dissertation/files/BasinATLAS/BasinATLAS_v10.gdb/'\n",
    "# set path where results will be stored\n",
    "path_to_save = '/home/dima/Documents/education/HSI/aspirantura/Dissertation/conus_data/featureXtractor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>geometry</th>\n",
       "      <th>AREA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01013500</td>\n",
       "      <td>MULTIPOLYGON (((-68.35650 46.90311, -68.35612 ...</td>\n",
       "      <td>2.303988e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01022500</td>\n",
       "      <td>POLYGON ((-67.97836 44.61310, -67.97800 44.613...</td>\n",
       "      <td>6.203873e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01030500</td>\n",
       "      <td>MULTIPOLYGON (((-67.83991 45.36614, -67.83955 ...</td>\n",
       "      <td>3.676155e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01031500</td>\n",
       "      <td>MULTIPOLYGON (((-69.33810 45.12317, -69.33800 ...</td>\n",
       "      <td>7.665447e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01047000</td>\n",
       "      <td>POLYGON ((-70.10847 45.21669, -70.10858 45.216...</td>\n",
       "      <td>9.049562e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>14309500</td>\n",
       "      <td>POLYGON ((-123.81322 42.89103, -123.81312 42.8...</td>\n",
       "      <td>2.263143e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>14316700</td>\n",
       "      <td>POLYGON ((-122.49936 43.47688, -122.49972 43.4...</td>\n",
       "      <td>5.880250e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>14325000</td>\n",
       "      <td>POLYGON ((-124.07751 42.89822, -124.07716 42.8...</td>\n",
       "      <td>4.449257e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>14362250</td>\n",
       "      <td>POLYGON ((-123.15128 42.19624, -123.15118 42.1...</td>\n",
       "      <td>4.387790e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>14400000</td>\n",
       "      <td>POLYGON ((-124.02084 42.36178, -124.02073 42.3...</td>\n",
       "      <td>7.033865e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gauge_id                                           geometry          AREA\n",
       "0    01013500  MULTIPOLYGON (((-68.35650 46.90311, -68.35612 ...  2.303988e+09\n",
       "1    01022500  POLYGON ((-67.97836 44.61310, -67.97800 44.613...  6.203873e+08\n",
       "2    01030500  MULTIPOLYGON (((-67.83991 45.36614, -67.83955 ...  3.676155e+09\n",
       "3    01031500  MULTIPOLYGON (((-69.33810 45.12317, -69.33800 ...  7.665447e+08\n",
       "4    01047000  POLYGON ((-70.10847 45.21669, -70.10858 45.216...  9.049562e+08\n",
       "..        ...                                                ...           ...\n",
       "666  14309500  POLYGON ((-123.81322 42.89103, -123.81312 42.8...  2.263143e+08\n",
       "667  14316700  POLYGON ((-122.49936 43.47688, -122.49972 43.4...  5.880250e+08\n",
       "668  14325000  POLYGON ((-124.07751 42.89822, -124.07716 42.8...  4.449257e+08\n",
       "669  14362250  POLYGON ((-123.15128 42.19624, -123.15118 42.1...  4.387790e+07\n",
       "670  14400000  POLYGON ((-124.02084 42.36178, -124.02073 42.3...  7.033865e+08\n",
       "\n",
       "[671 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read shape file with geometry column\n",
    "big_shape = gpd.read_file(shape_file_path)\n",
    "big_shape = big_shape[['hru_id', 'geometry', 'AREA']]\n",
    "\n",
    "# rename column of gauge identification number to capital ID\n",
    "big_shape = big_shape.rename(columns={\"hru_id\": \"gauge_id\"})\n",
    "big_shape['gauge_id'] = ['0'+i if len(i) != 8\n",
    "                         else i \n",
    "                         for i in map(str, big_shape['gauge_id']) ]\n",
    "big_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_area(lats: list, lons: list, radius = 6378137):\n",
    "    \"\"\"\n",
    "    Computes area of spherical polygon, assuming spherical Earth. \n",
    "    Returns result in ratio of the sphere's area if the radius is specified.\n",
    "    Otherwise, in the units of provided radius.\n",
    "    lats and lons are in degrees.\n",
    "\n",
    "    Args:\n",
    "        lats (list): [description]\n",
    "        lons (list): [description]\n",
    "        radius (int, optional): [description]. Defaults to 6378137.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    from numpy import arctan2, cos, sin, sqrt, pi, power, append, diff, deg2rad\n",
    "    lats, lons = np.deg2rad(lats), np.deg2rad(lons)\n",
    "\n",
    "    # Line integral based on Green's Theorem, assumes spherical Earth\n",
    "\n",
    "    #close polygon\n",
    "    if lats[0]!=lats[-1]:\n",
    "        lats=append(lats, lats[0])\n",
    "        lons=append(lons, lons[0])\n",
    "\n",
    "    #colatitudes relative to (0,0)\n",
    "    a = sin(lats/2)**2 + cos(lats)* sin(lons/2)**2\n",
    "    colat = 2*arctan2( sqrt(a), sqrt(1-a) )\n",
    "\n",
    "    #azimuths relative to (0,0)\n",
    "    az = arctan2(cos(lats) * sin(lons), sin(lats)) % (2*pi)\n",
    "\n",
    "    # Calculate diffs\n",
    "    # daz = diff(az) % (2*pi)\n",
    "    daz = diff(az)\n",
    "    daz = (daz + pi) % (2 * pi) - pi\n",
    "\n",
    "    deltas=diff(colat)/2\n",
    "    colat=colat[0:-1]+deltas\n",
    "\n",
    "    # Perform integral\n",
    "    integrands = (1-cos(colat)) * daz\n",
    "\n",
    "    # Integrate \n",
    "    area = abs(sum(integrands))/(4*pi)\n",
    "\n",
    "    area = min(area,1-area)\n",
    "    if radius is not None: #return in units of radius\n",
    "        return area * 4 * pi* radius**2 / 10**6\n",
    "    else: #return in ratio of sphere total area\n",
    "        return area / 10**6\n",
    "    \n",
    "def select_big_from_MP(WS_geometry):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        WS_geometry ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    if type(WS_geometry) == MultiPolygon:\n",
    "        big_area = [polygon_area(lats = polygon.exterior.coords.xy[1],\n",
    "                                 lons = polygon.exterior.coords.xy[0]) \n",
    "                    for polygon in WS_geometry]\n",
    "        WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "    else:\n",
    "        WS_geometry = WS_geometry\n",
    "    return WS_geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попытка считать площадь каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clz_cl_smj_10</th>\n",
       "      <th>cls_cl_smj_49</th>\n",
       "      <th>cls_cl_smj_53</th>\n",
       "      <th>glc_cl_smj_4</th>\n",
       "      <th>pnv_cl_smj_4</th>\n",
       "      <th>wet_cl_smj_-9999</th>\n",
       "      <th>tbi_cl_smj_5</th>\n",
       "      <th>tec_cl_smj_351</th>\n",
       "      <th>fmh_cl_smj_5</th>\n",
       "      <th>fmh_cl_smj_7</th>\n",
       "      <th>fec_cl_smj_103</th>\n",
       "      <th>fec_cl_smj_121</th>\n",
       "      <th>lit_cl_smj_2</th>\n",
       "      <th>lit_cl_smj_3</th>\n",
       "      <th>lit_cl_smj_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.554313</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>99.542662</td>\n",
       "      <td>99.554313</td>\n",
       "      <td>99.554313</td>\n",
       "      <td>99.554313</td>\n",
       "      <td>99.554313</td>\n",
       "      <td>99.554313</td>\n",
       "      <td>99.31084</td>\n",
       "      <td>0.243472</td>\n",
       "      <td>99.31084</td>\n",
       "      <td>0.243472</td>\n",
       "      <td>99.166224</td>\n",
       "      <td>0.222338</td>\n",
       "      <td>0.16575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clz_cl_smj_10  cls_cl_smj_49  cls_cl_smj_53  glc_cl_smj_4  pnv_cl_smj_4  \\\n",
       "0      99.554313       0.011651      99.542662     99.554313     99.554313   \n",
       "\n",
       "   wet_cl_smj_-9999  tbi_cl_smj_5  tec_cl_smj_351  fmh_cl_smj_5  fmh_cl_smj_7  \\\n",
       "0         99.554313     99.554313       99.554313      99.31084      0.243472   \n",
       "\n",
       "   fec_cl_smj_103  fec_cl_smj_121  lit_cl_smj_2  lit_cl_smj_3  lit_cl_smj_11  \n",
       "0        99.31084        0.243472     99.166224      0.222338        0.16575  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_for_class = dict()\n",
    "\n",
    "for column in gdf.columns:\n",
    "    if 'cl' in column.split('_'):\n",
    "        for key, value in dict(gdf.groupby(\n",
    "            [column])['weight_area'].sum()).items():\n",
    "            dict_for_class[column+'_'+str(key)] = [value]\n",
    "\n",
    "pd.DataFrame.from_dict(dict_for_class)/find_POLY_area(gdf_your_WS)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/dima/Documents/education/HSI/aspirantura/Dissertation/conus_data/EA_LSTM/featureXtractor/camels_hydro.csv',\n",
    "            sep=';')['lka_pc_use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict()\n",
    "\n",
    "for i in range(len(big_shape)):\n",
    "    output[big_shape.loc[\n",
    "        i, 'gauge_id']] = featureXtractor(user_ws=big_shape.loc[\n",
    "            i, 'geometry'],\n",
    "            gdb_file_path=gdb_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geo_files_to_disk(shape_with_data, list_of_values, path_to_save):\n",
    "    \n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    \n",
    "    bool_array = list()\n",
    "    \n",
    "    for i in range(len(list_of_values)):\n",
    "        if type(list_of_values[i][0]) == float:\n",
    "            bool_array.append(False)\n",
    "        else:\n",
    "            bool_array.append(True)\n",
    "            \n",
    "    VALID_ID = [ID for i, ID in enumerate(shape_with_data.gauge_id) if bool_array[i]]\n",
    "\n",
    "    VALID_HYDRO = [hydro[0][0] for i, hydro in enumerate(list_of_values) if bool_array[i]]\n",
    "    hydro_df = pd.concat(VALID_HYDRO).dropna().reset_index(drop = True)\n",
    "    hydro_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    hydro_df.to_csv('{}/camels_hydro.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_PHYSIO = [physio[0][1] for i, physio in enumerate(list_of_values) if bool_array[i]]\n",
    "    physio_df = pd.concat(VALID_PHYSIO).dropna().reset_index(drop = True)\n",
    "    physio_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    physio_df.to_csv('{}/camels_physio.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_CLIMATE = [climate[0][2] for i, climate in enumerate(list_of_values) if bool_array[i]]\n",
    "    climate_df = pd.concat(VALID_CLIMATE).dropna().reset_index(drop = True)\n",
    "    climate_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    climate_df.to_csv('{}/camels_climate.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_URBAN = [urban[0][5] for i, urban in enumerate(list_of_values) if bool_array[i]]\n",
    "    urban_df = pd.concat(VALID_URBAN).dropna().reset_index(drop = True)\n",
    "    urban_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    urban_df.to_csv('{}/camels_urban.csv'.format(path_to_save), index = False, sep=';')\n",
    "    #because of big empty sets of WS Landcover is seleceting by indexes of urban values which are verified\n",
    "    VALID_LANDCOVER = [landcover[0][3] for i, landcover in enumerate(list_of_values) if bool_array[i]] \n",
    "    landcover_df = pd.concat(VALID_LANDCOVER).dropna(thresh = 5).reset_index(drop = True)\n",
    "    landcover_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    landcover_df.to_csv('{}/camels_landcover.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_SOIL_GEO = [soil_geo[0][4] for i, soil_geo in enumerate(list_of_values) if bool_array[i]]\n",
    "    soil_geo_df = pd.concat(VALID_SOIL_GEO).dropna().reset_index(drop = True)\n",
    "    soil_geo_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    soil_geo_df.to_csv('{}/camels_soil_geo.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_GEOM_HydroATLAS = [geometry[1] for i, geometry in enumerate(list_of_values) if bool_array[i]]\n",
    "    geometry_df = gpd.GeoDataFrame(VALID_GEOM_HydroATLAS,\n",
    "                                   columns={'geometry'}).reset_index(drop = True)\n",
    "    geometry_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "\n",
    "    geometry_df.to_csv('{}/geometry_HydroATLAS_subB.csv'.format(path_to_save), index = False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_HydroATLAS_sub_basins(WS_own, HydroATLAS_data):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    WS_own - Watershed from your GDF of watersheds\n",
    "    HydroATLAS_data - gdf file from layers of geodatabase\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    intersected_sub_basins = list()\n",
    "\n",
    "    def select_big_from_MP(WS_geometry):\n",
    "        if type(WS_geometry) == MultiPolygon:\n",
    "            big_area = [polygon_area(lats = polygon.exterior.coords.xy[1], \n",
    "                                    lons = polygon.exterior.coords.xy[0]) \n",
    "                        for polygon in WS_geometry]\n",
    "            import numpy as np\n",
    "            WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "        else:\n",
    "            WS_geometry = WS_geometry\n",
    "        return WS_geometry\n",
    "\n",
    "    gdf_your_WS = select_big_from_MP(WS_own)\n",
    "    ### WS from your data\n",
    "    gdf_your_WS = gpd.GeoSeries([gdf_your_WS])\n",
    "\n",
    "    ### Create extra gdf to use geopandas functions\n",
    "    gdf_your_WS = gpd.GeoDataFrame({'geometry': gdf_your_WS})\n",
    "    gdf_your_WS = gdf_your_WS.set_crs('EPSG:4326')\n",
    "\n",
    "    for HydroATLAS_row in range(len(HydroATLAS_data)):\n",
    "\n",
    "        # selection from sub-basins of GeoDataBase\n",
    "        HydroATLAS_WS = gpd.GeoSeries(select_big_from_MP(HydroATLAS_data.geometry[HydroATLAS_row]))        \n",
    "\n",
    "        gdf_HydroATLAS_WS = gpd.GeoDataFrame({'geometry': HydroATLAS_WS}).set_crs('EPSG:4326')\n",
    "\n",
    "        #intersect basins\n",
    "        res_intersection = gpd.overlay(gdf_your_WS, gdf_HydroATLAS_WS, how='intersection')\n",
    "\n",
    "        \"\"\"\n",
    "        Check if our intersection between sub-basin form HydroAtlas and our watershed is more than 0.6 of \n",
    "        sub-basin itself\n",
    "        If not - than pass        \n",
    "        \"\"\"\n",
    "        if len(res_intersection) != 0:\n",
    "            res_intersection = select_big_from_MP(res_intersection.geometry[0])\n",
    "\n",
    "            if polygon_area(lats = res_intersection.exterior.coords.xy[1], \n",
    "                            lons = res_intersection.exterior.coords.xy[0])/(\n",
    "                polygon_area(lats = gdf_HydroATLAS_WS.geometry[0].exterior.coords.xy[1],\n",
    "                             lons = gdf_HydroATLAS_WS.geometry[0].exterior.coords.xy[0])) > 0.2:\n",
    "                    \n",
    "\n",
    "                    intersected_sub_basins.append(HydroATLAS_data.loc[HydroATLAS_row])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return intersected_sub_basins\n",
    "\n",
    "\n",
    "def split_by_categories(df_ecm, df_me, df_mo):\n",
    "    \n",
    "    # basic numbers for different variables\n",
    "    monthes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    land_cover_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
    "    natural_vegetation = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "    wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "    \n",
    "    hydrology_variables = [item for sublist in [['inu_pc_ult'], ['lka_pc_use'], ['lkv_mc_usu'],\n",
    "                                            ['rev_mc_usu'], ['dor_pc_pva'], ['gwt_cm_sav']]\n",
    "                    for item in sublist]\n",
    "\n",
    "    physiography_variables = [item for sublist in [['ele_mt_sav'], ['slp_dg_sav'], ['sgr_dk_sav']] \n",
    "                            for item in sublist]\n",
    "\n",
    "    climate_variables = [item for sublist in [['clz_cl_smj'], ['cls_cl_smj'], ['tmp_dc_s{}'.format(i) for i in monthes], \n",
    "                                            ['pre_mm_s{}'.format(i) for i in monthes], ['pet_mm_s{}'.format(i) for i in monthes],\n",
    "                                            ['aet_mm_s{}'.format(i) for i in monthes], ['ari_ix_sav'],\n",
    "                                            ['cmi_ix_s{}'.format(i) for i in monthes], ['snw_pc_s{}'.format(i) for i in monthes]] \n",
    "                        for item in sublist]\n",
    "\n",
    "    landcover_variables = [item for sublist in [['glc_cl_smj'], ['glc_pc_s{}'.format(i) for i in land_cover_classes], \n",
    "                                                ['pnv_cl_smj'], ['wet_cl_smj'], ['wet_pc_s{}'.format(i) for i in wetland_classes],\n",
    "                                                ['for_pc_sse'], ['crp_pc_sse'], ['pst_pc_sse'], \n",
    "                                                ['ire_pc_sse'], ['gla_pc_sse'], ['prm_pc_sse'], \n",
    "                                                ['tbi_cl_smj'], ['tec_cl_smj']]\n",
    "                        for item in sublist]\n",
    "\n",
    "    soil_and_geo_variables = [item for sublist in [['cly_pc_sav'], ['slt_pc_sav'], ['snd_pc_sav'], \n",
    "                                                ['soc_th_sav'], ['swc_pc_syr'], ['swc_pc_s{}'.format(i) for i in monthes],\n",
    "                                                ['lit_cl_smj'], ['kar_pc_sse'], ['ero_kh_sav']]\n",
    "                            for item in sublist]\n",
    "\n",
    "    urban_variables = [item for sublist in [['urb_pc_sse'], ['hft_ix_s93'], ['hft_ix_s09']] for item in sublist]\n",
    "\n",
    "    # dataframe of hydrology variables\n",
    "    df_HYDRO = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in hydrology_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in hydrology_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in hydrology_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of physiography variables\n",
    "    df_PHYSIO = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in physiography_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in physiography_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in physiography_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "\n",
    "    # dataframe of climate variables\n",
    "    df_CLIMATE = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in climate_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in climate_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in climate_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of physiography variables                       \n",
    "    df_LANDCOVER = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in landcover_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in landcover_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in landcover_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of soil and geology variables\n",
    "    df_SOIL_GEO = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in soil_and_geo_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in soil_and_geo_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in soil_and_geo_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    # dataframe of urban variables\n",
    "    df_URBAN = pd.concat([\n",
    "                            df_ecm[\n",
    "                            df_ecm.columns[\n",
    "                            [True if i in urban_variables else False for i in df_ecm.columns]\n",
    "                                                    ]],\n",
    "                            df_me[\n",
    "                            df_me.columns[\n",
    "                            [True if i in urban_variables else False for i in df_me.columns]\n",
    "                                                    ]],\n",
    "                            df_mo[\n",
    "                            df_mo.columns[\n",
    "                            [True if i in urban_variables else False for i in df_mo.columns]\n",
    "                                                    ]]\n",
    "                            ], axis = 1)\n",
    "    return [df_HYDRO, df_PHYSIO, df_CLIMATE, df_LANDCOVER, df_SOIL_GEO, df_URBAN]\n",
    "\n",
    "def get_HydroATLAS_for_WS(WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS):\n",
    "\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    def select_big_from_MP(WS_geometry):\n",
    "        if type(WS_geometry) == MultiPolygon:\n",
    "            big_area = [polygon_area(lats = polygon.exterior.coords.xy[1], \n",
    "                                    lons = polygon.exterior.coords.xy[0]) \n",
    "                        for polygon in WS_geometry]\n",
    "            WS_geometry = WS_geometry[np.argmax(big_area)]\n",
    "        else:\n",
    "            WS_geometry = WS_geometry\n",
    "        return WS_geometry\n",
    "    \n",
    "    \"\"\"                             \n",
    "    WS - Your data with watershed GDF.geometry[:]\n",
    "    WS_index - Number of the index of WS from GeoDataFrame geometry field\n",
    "    path_to_HydroATLAS - Path to BasinATLAS_v10.gdb file\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # basic numbers for different variables\n",
    "    monthes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    land_cover_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n",
    "    natural_vegetation = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
    "    wetland_classes = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "    \n",
    "    # Get all the layers from the .gdb file \n",
    "    layers = layers_from_HydroATLAS\n",
    "    # -1 layer - high density sub-basins (lowest area)\n",
    "    \n",
    "    # Read choosen geodatabase layer with geopandas\n",
    "    gdf = gpd.read_file(path_to_HydroATLAS, \n",
    "                        mask = WS.geometry[WS_index], layer=layers,  ignore_geometry=False)\n",
    "    \n",
    "    \n",
    "    list_of_goodies = filter_HydroATLAS_sub_basins(WS.geometry[WS_index], gdf)\n",
    "  \n",
    "    if len(list_of_goodies) != 0:\n",
    "        list_of_goodies = gpd.GeoDataFrame(pd.DataFrame(list_of_goodies)).set_crs('EPSG:4326').reset_index(drop = True)\n",
    "        from shapely.ops import unary_union\n",
    "        union_geometry = gpd.GeoSeries(unary_union([i for i in list_of_goodies.geometry])).set_crs('epsg:4326')\n",
    "        union_geometry = select_big_from_MP(union_geometry.geometry[0])\n",
    "\n",
    "        \"\"\"\n",
    "        group columns by category (difference is the way of mathematical aggregation)\n",
    "\n",
    "        e.g. classes will be aggrgated by mode value in sub-basins of the watershed\n",
    "\n",
    "        other values will be calculated as a mean for selected watershed\n",
    "\n",
    "        \"\"\"\n",
    "        # values which will be aggregated by mean\n",
    "        columns_MEAN = [['inu_pc_ult'], ['lka_pc_use'], ['lkv_mc_usu'], ['rev_mc_usu'], ['dor_pc_pva'], ['gwt_cm_sav'], ['ele_mt_sav'], ['slp_dg_sav'],\n",
    "                ['sgr_dk_sav'], ['tmp_dc_s{}'.format(i) for i in monthes], ['pre_mm_s{}'.format(i) for i in monthes], \n",
    "                ['pet_mm_s{}'.format(i) for i in monthes], ['aet_mm_s{}'.format(i) for i in monthes], ['snw_pc_s{}'.format(i) for i in monthes], \n",
    "                ['glc_pc_s{}'.format(i) for i in land_cover_classes], ['pnv_pc_s{}'.format(i) for i in natural_vegetation], ['wet_pc_s{}'.format(i) for i in wetland_classes], \n",
    "                ['for_pc_sse'], ['crp_pc_sse'], ['pst_pc_sse'], ['ire_pc_sse'], ['gla_pc_sse'], ['prm_pc_sse'], ['cly_pc_sav'], ['slt_pc_sav'], \n",
    "                ['snd_pc_sav'], ['soc_th_sav'], ['swc_pc_syr'], ['swc_pc_s{}'.format(i) for i in monthes], ['kar_pc_sse'], ['ero_kh_sav'], ['urb_pc_sse']]\n",
    "\n",
    "        # values which will be aggregated by mode\n",
    "        columns_MODE = [['clz_cl_smj'], ['cls_cl_smj'], ['glc_cl_smj'], ['pnv_cl_smj'],\n",
    "                        ['wet_cl_smj'], ['tbi_cl_smj'], ['tec_cl_smj'], ['lit_cl_smj']]\n",
    "\n",
    "        # values which will be aggregated by mean but need extra calculations\n",
    "        # e.g. ari_ix need to be divided by 10, cmi by 100 etc.\n",
    "        # for some reason values exceed treshold of the range\n",
    "        columns_EXTRA_CALC_MEAN = [['ari_ix_sav'], ['cmi_ix_s{}'.format(i) for i in monthes], ['hft_ix_s93'], ['hft_ix_s09']]\n",
    "\n",
    "        # split list of lists to needed columns\n",
    "        columns_EXTRA_CALC_MEAN = [item for sublist in columns_EXTRA_CALC_MEAN for item in sublist]\n",
    "        columns_MEAN = [item for sublist in columns_MEAN for item in sublist]\n",
    "        columns_MODE = [item for sublist in columns_MODE for item in sublist]\n",
    "        \n",
    "        # dataframe for indexes\n",
    "\n",
    "        df_EXTRA_CALC_MEAN = list_of_goodies[columns_EXTRA_CALC_MEAN]\n",
    "        df_EXTRA_CALC_MEAN.loc[:, ['ari_ix_sav']] /= 10 # aridity index is the value between 0 and 100. In current version of HydroATLAS (v 1.0) it's vary between 0 and 1000\n",
    "        \n",
    "        df_EXTRA_CALC_MEAN.loc[:, ['cmi_ix_s{}'.format(i) for i in monthes]] /= 100 # aridity index is the value between -1 and 1. In current version of HydroATLAS (v 1.0) it's vary between -100 and 100\n",
    "\n",
    "        df_EXTRA_CALC_MEAN = df_EXTRA_CALC_MEAN.mean()\n",
    "        \n",
    "        # dataframe for area values\n",
    "\n",
    "        df_MEAN = list_of_goodies[columns_MEAN]\n",
    "        df_MEAN.loc[:, ['tmp_dc_s{}'.format(i) for i in monthes]] /= 10 # in some regions on North-West Russia average value for Jan -83. I assume it's need to be divide by 10\n",
    "        df_MEAN = df_MEAN.mean()\n",
    "        \n",
    "\n",
    "        #dataframe for classes\n",
    "\n",
    "        df_MODE = list_of_goodies[columns_MODE]\n",
    "        df_MODE = df_MODE.replace(-9999, np.NaN) # Это вопрос: может стоит оставить \"отсутствующий класс\" \"мокрых земель\"\n",
    "        df_MODE = df_MODE.mode()\n",
    "        \n",
    "        \n",
    "        list_of_frames = [df_EXTRA_CALC_MEAN, df_MEAN, df_MODE]\n",
    "        \n",
    "        for i in range(len(list_of_frames)):\n",
    "            if type(list_of_frames[i]) == pd.Series:\n",
    "                list_of_frames[i] = list_of_frames[i].to_frame().T\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        fin = split_by_categories(df_ecm = list_of_frames[0], df_me = list_of_frames[1], df_mo = list_of_frames[2])\n",
    "    \n",
    "    else:\n",
    "        list_of_goodies = np.NaN\n",
    "        union_geometry = np.NaN\n",
    "        fin = np.NaN \n",
    "    \n",
    "    \n",
    "    return fin, union_geometry, list_of_goodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_function(WS, path_to_HydroATLAS, layer_small):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function generate list of tuples\n",
    "    where each tuple stands for row in DF\n",
    "    of watersheds\n",
    "    WS - GeoDataFrame of WS\n",
    "    path_to_HydroATLAS - path to BasinATLAS gdb\n",
    "    layer_small - fiona layer of smallest grid of WS    \n",
    "    \"\"\"\n",
    "    mp_tuples = list()\n",
    "    path_to_HydroATLAS = path_to_HydroATLAS\n",
    "    \n",
    "    for row in range(len(WS)):\n",
    "        mp_tuples.append((WS,\n",
    "                          row,\n",
    "                          path_to_HydroATLAS,\n",
    "                          layer_small))\n",
    "    \n",
    "    return mp_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get layers of smallest scale sub-basins\n",
    "layer_small = fiona.listlayers(gdb_file_path)[-1]\n",
    "# WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS\n",
    "data = parallelize_function(WS = big_shape,\n",
    "                            path_to_HydroATLAS = gdb_file_path,\n",
    "                            layer_small=layer_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee2b62a5b044d18a8e9eaae923367ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data and iterations on test example\n",
    "\n",
    "# get count of cpu. Substract by 2 to not to overwhelm system\n",
    "function_processors = mp.cpu_count()//2\n",
    "\n",
    "process_pool = mp.Pool(function_processors)\n",
    "# WS, WS_index, path_to_HydroATLAS, layers_from_HydroATLAS\n",
    "output = process_pool.starmap(get_HydroATLAS_for_WS, tq.tqdm(data))\n",
    "process_pool.close()\n",
    "process_pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_geo_files_to_disk(shape_with_data, list_of_values, path_to_save):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function translate resulted list\n",
    "    to csv separated by category of variables\n",
    "    which are they store    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    \n",
    "    bool_array = list()\n",
    "    \n",
    "    for i in range(len(list_of_values)):\n",
    "        if type(list_of_values[i][0]) == float:\n",
    "            bool_array.append(False)\n",
    "        else:\n",
    "            bool_array.append(True)\n",
    "            \n",
    "    VALID_ID = [ID for i, ID in enumerate(shape_with_data.gauge_id) if bool_array[i]]\n",
    "\n",
    "    VALID_HYDRO = [hydro[0][0] for i, hydro in enumerate(list_of_values) if bool_array[i]]\n",
    "    hydro_df = pd.concat(VALID_HYDRO).dropna().reset_index(drop = True)\n",
    "    hydro_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    hydro_df.to_csv('{}/camels_hydro.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_PHYSIO = [physio[0][1] for i, physio in enumerate(list_of_values) if bool_array[i]]\n",
    "    physio_df = pd.concat(VALID_PHYSIO).dropna().reset_index(drop = True)\n",
    "    physio_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    physio_df.to_csv('{}/camels_physio.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_CLIMATE = [climate[0][2] for i, climate in enumerate(list_of_values) if bool_array[i]]\n",
    "    climate_df = pd.concat(VALID_CLIMATE).dropna().reset_index(drop = True)\n",
    "    climate_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    climate_df.to_csv('{}/camels_climate.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_URBAN = [urban[0][5] for i, urban in enumerate(list_of_values) if bool_array[i]]\n",
    "    urban_df = pd.concat(VALID_URBAN).dropna().reset_index(drop = True)\n",
    "    urban_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    urban_df.to_csv('{}/camels_urban.csv'.format(path_to_save), index = False, sep=';')\n",
    "    #because of big empty sets of WS Landcover is seleceting by indexes of urban values which are verified\n",
    "    VALID_LANDCOVER = [landcover[0][3] for i, landcover in enumerate(list_of_values) if bool_array[i]] \n",
    "    landcover_df = pd.concat(VALID_LANDCOVER).dropna(thresh = 5).reset_index(drop = True)\n",
    "    landcover_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    landcover_df.to_csv('{}/camels_landcover.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_SOIL_GEO = [soil_geo[0][4] for i, soil_geo in enumerate(list_of_values) if bool_array[i]]\n",
    "    soil_geo_df = pd.concat(VALID_SOIL_GEO).dropna().reset_index(drop = True)\n",
    "    soil_geo_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "    soil_geo_df.to_csv('{}/camels_soil_geo.csv'.format(path_to_save), index = False, sep=';')\n",
    "\n",
    "    VALID_GEOM_HydroATLAS = [geometry[1] for i, geometry in enumerate(list_of_values) if bool_array[i]]\n",
    "    geometry_df = gpd.GeoDataFrame(VALID_GEOM_HydroATLAS,\n",
    "                                   columns={'geometry'}).reset_index(drop = True)\n",
    "    geometry_df.insert(loc = 0, column = 'gauge_id', value = VALID_ID)\n",
    "\n",
    "    geometry_df.to_csv('{}/geometry_HydroATLAS_subB.csv'.format(path_to_save), index = False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_geo_files_to_disk(shape_with_data=big_shape, \n",
    "                       list_of_values=output,\n",
    "                       path_to_save=path_to_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb0a400ccb40e90b44d0772e946b0e66d4b8c1ba2d10d17ce44700eafd995fb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
